/*
* Copyright 2021 Google LLC

* Licensed under the Apache License, Version 2.0 (the "License");
* you may not use this file except in compliance with the License.
* You may obtain a copy of the License at

*     https://www.apache.org/licenses/LICENSE-2.0

* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an "AS IS" BASIS,
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
* See the License for the specific language governing permissions and
* limitations under the License.
*/

const clamd = require('clamdjs');
const express = require('express');
const {PubSub} = require('@google-cloud/pubsub');
const {Storage} = require('@google-cloud/storage');
const {ApiError} = require('@google-cloud/common');
const {GoogleAuth} = require('google-auth-library');
const {logger} = require('./logger.js');
const pkgJson = require('./package.json');
const metrics = require('./metrics.js');

const PORT = process.env.PORT || 8080;
const CLAMD_HOST = '127.0.0.1';
const CLAMD_PORT = 3310;

// 10 min timeout for scanning.
const CLAMD_TIMEOUT = 600000;

// Note: MAX_FILE_SIZE limits the size of files which are sent to th
// ClamAV Daemon.
//
// ClamAV itself has internal limits, which apply both to the total file
// size, and to the size of compressed files inside file containers.
// These are set in the clamd.conf file by bootstrap.sh
//
// Note scanning a 500MiB file can take 5 minutes, so ensure timeout is
// large enough.
const MAX_FILE_SIZE = 500000000; // 500MiB

/**
 * Configuration object.
 *
 * Values are read from the environment variable CONFIG_FILE (which specifies a
 * JSON file to read the config from) or single-bucket config variables:
 * UPLOAD_TOPIC, UNSCANNED_BUCKET, CLEAN_BUCKET and QUARANTINED_BUCKET.
 * See {@link readAndVerifyConfig}.
 *
 * @type {{
 *    buckets: Array<
 *      {
 *        unscanned: string,
 *        clean: string,
 *        quarantined: string
 *       }>,
 *    upload_topic: string
 *  }}
 */
const CONFIG = {
  buckets: [],
  upload_topic: '',
};

// Create Clients.
const app = express();
app.use(express.json());
const scanner = clamd.createScanner(CLAMD_HOST, CLAMD_PORT);
const storage = new Storage();

/**
 * Create a subscription to the upload topic.
 *
 * Upload topic is a Pub/Sub topic which is updated whenever a new file is
 * uploaded to cloud storage bucket & needs to be scanned. Message format:
 * https://cloud.google.com/storage/docs/pubsub-notifications
 * @param {string} projectId Google Cloud Platform project ID
 * @return {Promise<void>}
 */
async function subscribeToUploadTopic(
    projectId = 'your-project-id',
) {
  const pubsub = new PubSub({projectId: projectId});

  // Get subscriptions for upload topic, and subscribe to one
  const [subscriptions] = await pubsub
      .topic(CONFIG.upload_topic)
      .getSubscriptions();

  logger.info('Subscriptions on upload topic:');
  subscriptions.forEach((sub) => console.log(sub.name));
  const subscription = subscriptions[0];
  logger.info('Subscribing to ' + subscription.name);

  // Receive callbacks for new messages on the subscription
  subscription.on('message', (message) => {
    logger.info('Received message ${message.id}: ${message}');
    onFileUpload(message);
    message.ack();
  });

  // Receive callbacks for errors on the subscription
  subscription.on('error', (error) => {
    logger.error('Received error from Upload Topic:', error);
  });

  // Send a message to the topic
  // topic.publish(Buffer.from('Test message!'));
}

/**
 * Route that is invoked by Cloud Run when a malware scan is requested
 * for a document uploaded to GCS.
 *
 * For command line testing, use
 *
 * curl -d '{"kind": "storage#object","name":"sparse_file_1G", "bucket": "BUCKET_NAME" }' -H "Content-Type: application/json" http://localhost:8080
 *
 *
 * @param {Object} message The request payload
 */
async function onFileUpload(message) {
  // Sanity check required values.
  const file = JSON.parse(message.data);
  if (file.kind !== 'storage#object') {
    logger.error(`${file} is not a GCS Storage Object`);
    return;
  }

  try {
    if (!file.name) {
      logger.error(`file name not specified in ${file}`);
      return;
    }
    if (!file.bucket) {
      logger.error(`bucket name not specified in ${file}`);
      return;
    }
    if (file.size > MAX_FILE_SIZE) {
      logger.error(`file gs://${file.bucket}/${file.name} too large for scanning at ${
        file.size} bytes`);
      return;
    }
    const config =
        CONFIG.buckets.filter((c) => c.unscanned === file.bucket)[0];
    if (!config) {
      logger.error(`Bucket name - ${file.bucket} not in config`);
      return;
    }

    const gcsFile = storage.bucket(file.bucket).file(file.name);
    // File.exists() returns a FileExistsResponse, which is a list with a
    // single value.
    if (! (await gcsFile.exists())[0]) {
      logger.warn(`File: gs://${file.bucket}/${file.name} does not exist. It may have already been processed.`);
      return;
    }

    const clamdVersion = await getClamVersion();
    logger.info(`Scan request for gs://${file.bucket}/${file.name}, (${
      file.size} bytes) scanning with clam ${clamdVersion}`);
    const startTime = Date.now();
    const readStream = await gcsFile.createReadStream();
    let result;
    try {
      result = await scanner.scanStream(readStream, CLAMD_TIMEOUT);
    } finally {
      // Ensure stream is destroyed in all situations to prevent any
      // resource leaks.
      readStream.destroy();
    }
    const scanDuration = Date.now() - startTime;

    if (clamd.isCleanReply(result)) {
      logger.info(`Scan status for gs://${file.bucket}/${file.name}: CLEAN (${
        file.size} bytes in ${scanDuration} ms)`);
      metrics.writeScanClean(config.unscanned, config.clean, file.size,
          scanDuration, clamdVersion);

      // Move document to the bucket that holds clean documents. This can
      // fail due to permissions or if the file has been deleted.
      await moveProcessedFile(file.name, true, config);
    } else {
      logger.warn(`Scan status for gs://${file.bucket}/${
        file.name}: INFECTED ${result} (${
        file.size} bytes in ${scanDuration} ms)`);
      metrics.writeScanInfected(config.unscanned, config.quarantined,
          file.size, scanDuration, clamdVersion);

      // Move document to the bucket that holds infected documents. This can
      // fail due to permissions or if the file has been deleted.
      await moveProcessedFile(file.name, false, config);
    }
  } catch (e) {
    logger.error(
        {err: e},
        `Exception when processing gs://${file.bucket}/${file.name}: %s`,
        e.message);
  }
}

/**
 * Trivial handler for get requests which returns the clam version.
 *
 * Use:
 * curl -D - -H "Authorization: Bearer $(gcloud auth print-identity-token)"  \
     CLOUD_RUN_APP_URL
 * @param {!Request} req
 * @param {!Response} res
 */
app.get('/', async (req, res) => {
  res.status(200)
      .type('text/plain')
      .send(
          `${pkgJson.name} version ${pkgJson.version}
Using Clam AV version: ${await getClamVersion()}

${pkgJson.description}
(Responds to POST requests containing a GCS object only)

`);
});

/**
 * Respond with an error and log the message
 *
 * Note: any non-successful status codes will cause the caller (PubSub/Eventarc)
 * to retry sending the event, so use 200 for non-retryable errors.
 *
 * @param {Object} res response object
 * @param {number} statusCode
 * @param {string} errorMessage
 * @param {string=} unscannedBucket
 */
function handleErrorResponse(res, statusCode, errorMessage,
    unscannedBucket = /** @type {string} */ null) {
  logger.error(`Error processing request: ${errorMessage}`);
  res.status(statusCode).json({message: errorMessage, status: 'error'});
  metrics.writeScanFailed(unscannedBucket);
}

/**
 * Wrapper to get a clean string with the version of CLAM.
 * @return {Promise<string>}
 */
async function getClamVersion() {
  return (await clamd.version(CLAMD_HOST, CLAMD_PORT)).replace('\x00', '');
}

/**
 * Move the file to the appropriate bucket.
 * @param {string} filename
 * @param {boolean} isClean
 * @param {!Object} config
 */
async function moveProcessedFile(filename, isClean, config) {
  const srcfile = storage.bucket(config.unscanned).file(filename);
  const destinationBucketName =
      isClean ? `gs://${config.clean}` : `gs://${config.quarantined}`;
  const destinationBucket = storage.bucket(destinationBucketName);
  await srcfile.move(destinationBucket);
}

/**
 * Read configuration from process environmental variables.
 *
 * Can be CONFIG_FILE for a JSON file, or single-bucket config
 * variables: UNSCANNED_BUCKET, CLEAN_BUCKET and
 * QUARANTINED_BUCKET
 */
async function readAndVerifyConfig() {
  if (process.env.CONFIG_FILE) {
    if (!process.env.CONFIG_FILE.endsWith('.json')) {
      throw new Error(`CONFIG_FILE="${
        process.env.CONFIG_FILE}" should end with ".json"`);
    }
    try {
      const envConfig = require(process.env.CONFIG_FILE);
      CONFIG.buckets = envConfig.buckets;
      CONFIG.upload_topic = envConfig.upload_topic;
    } catch (e) {
      logger.fatal(
          {err: e},
          `Unable to read JSON file from CONFIG_FILE="${
            process.env.CONFIG_FILE}"`);
      throw new Error(`Invalid configuration CONFIG_FILE="${
        process.env.CONFIG_FILE}"`);
    }
  } else if (process.env.UNSCANNED_BUCKET && process.env.CLEAN_BUCKET &&
      process.env.QUARANTINED_BUCKET && process.env.UPLOAD_TOPIC) {
    CONFIG.upload_topic = process.env.UPLOAD_TOPIC;

    // Simple config for single-bucket scanning.
    CONFIG.buckets = [{
      unscanned: process.env.UNSCANNED_BUCKET,
      clean: process.env.CLEAN_BUCKET,
      quarantined: process.env.QUARANTINED_BUCKET,
    }];
  }

  if (!CONFIG.upload_topic) {
    logger.fatal('No topic configured for uploaded files');
    throw new Error('No topic configured');
  }

  if (CONFIG.buckets.length === 0) {
    logger.fatal(`No buckets configured for scanning`);
    throw new Error('No buckets configured');
  }

  logger.info('CONFIG: '+JSON.stringify(CONFIG, null, 2));

  let success = true;
  for (let x = 0; x < CONFIG.buckets.length; x++) {
    const config = CONFIG.buckets[x];
    for (const bucketType of ['unscanned', 'clean', 'quarantined']) {
      const bucketName = config[bucketType];
      if (!bucketName) {
        logger.fatal(`Error in bucket config[${x}]: no "${
          bucketType}" bucket defined`);
        success = false;
      }
      // Check for bucket existence by listing files in bucket, will throw
      // an exception if the bucket is not readable.
      // This is used in place of Bucket.exists() to avoid the need for
      // Project/viewer permission.
      try {
        await storage.bucket(bucketName).getFiles(
            {maxResults: 1, prefix: 'zzz', autoPaginate: false});
      } catch (e) {
        logger.fatal(`Error in bucket config[${x}]: cannot view files in "${
          bucketType}" bucket: ${bucketName} : ${e.message}`);
        logger.debug({err: e});
        success = false;
      }
    }
    if (config.unscanned === config.clean ||
        config.unscanned === config.quarantined ||
        config.clean === config.quarantined) {
      logger.fatal(
          `Error in bucket config[${x}]: bucket names are not unique`);
      success = false;
    }
  }
  if (!success) {
    throw new Error('Invalid configuration');
  }
}

/**
 * Perform async setup and start the app.
 */
async function run() {
  let projectId = process.env.PROJECT_ID;
  if (!projectId) {
    // Metrics needs project ID, so get it from GoogleAuth
    projectId = await (new GoogleAuth().getProjectId());
  }
  await metrics.init(projectId);
  await readAndVerifyConfig();
  await subscribeToUploadTopic(projectId);

  app.listen(PORT, () => {
    logger.info(
        `${pkgJson.name} version ${pkgJson.version} started on port ${PORT}`);
  });
}

// Start the service, exiting on error.
run().catch((e) => {
  logger.fatal(e);
  logger.fatal('Exiting');
  process.exit(1);
});
